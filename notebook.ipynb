# -*- coding: utf-8 -*-
"""C579_FP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kd2njlhXPwWAdgAIZt9Il70Xa7BohzWY

# simple_model_mm.py
"""

import random

import gym
import numpy as np
from scipy.linalg import expm  # O(n^3)


class SimpleEnv(gym.Env):

    def __init__(self,
                 T=10, dt=1, Q=3, dq=1, Q_0=0, dp=0.1, min_dp=1, mu=100, std=0.01, lambda_pos=1,
                 lambda_neg=1, kappa=100, alpha=1e-4, phi=1e-5, pre_run=None, printing=False, debug=False, d=5,
                 use_all_times=True, analytical=False, breaching_penalty=False, breach_penalty=20,
                 reward_scale=1, breach_penalty_function=np.square):
        super(SimpleEnv, self).__init__()

        self.T = T             
        self.dt = dt            

        self.Q = Q             
        self.dq = dq           
        self.Q_0 = Q_0         

        self.dp = dp          
        self.min_dp = min_dp    
        self.d = d             

        self.mu = mu            
        self.std = std         

        self.alpha = alpha     
        self.phi = phi        

        self.use_all_times = use_all_times

        # Set the action and observation space
        self.set_action_space()
        self.set_observation_space()

        self.lambda_pos = lambda_pos
        self.lambda_neg = lambda_neg
        self.kappa = kappa
        self.mm_bid = None
        self.mm_ask = None

        self.z = None
        self.A = None
        self.init_analytically_optimal()

        self.printing = printing
        self.debug = debug

        self.analytical = analytical
        self.breach_penalty = breach_penalty
        self.breach = False
        self.breaching_penalty = breaching_penalty
        self.breach_penalty_function = breach_penalty_function

        self.reward_scale = reward_scale

        # Reset the environment
        self.reset()

        if pre_run != None:
            self.pre_run(pre_run)  

        # Remembering the start price for the reward
        self.start_mid = self.mid


    def set_action_space(self):

        low = np.array([0, 0])
        high = np.array([self.d - 1, self.d - 1])  

        self.action_space = gym.spaces.Box(low=low, high=high, dtype=np.int16)


    def set_observation_space(self):

        if self.use_all_times:
            low = np.array([- self.Q, 0])
            high = np.array([self.Q, self.T / self.dt])
        else:
            low = np.array([- self.Q, 0])
            high = np.array([self.Q, 1])

        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.int16)


    def state(self):

        if self.use_all_times:
            return self.Q_t, self.t
        else:
            return self.Q_t, int(self.t / self.T >= 0.9)


    def pre_run(self, n_steps=100):

        for _ in range(n_steps):
            self.update_price()


    def update_price(self):

        self.mid += self.round_to_tick(np.random.normal(0, self.std))


    def init_analytically_optimal(self):

        self.z = np.exp(-self.alpha * self.kappa * np.square(np.array(range(self.Q, -self.Q - 1, -1))))
        self.A = np.zeros((self.Q * 2 + 1, self.Q * 2 + 1))
        for i in range(-self.Q, self.Q + 1):
            for q in range(-self.Q, self.Q + 1):
                if i == q:
                    self.A[i + self.Q, q + self.Q] = -self.phi * self.kappa * (q ** 2)
                elif i == q - 1:
                    self.A[i + self.Q, q + self.Q] = self.lambda_pos * np.exp(-1)
                elif i == q + 1:
                    self.A[i + self.Q, q + self.Q] = self.lambda_neg * np.exp(-1)


    def calc_analytically_optimal(self):

        omega = np.matmul(expm(self.A * (self.T - self.t)), self.z)
        h = 1 / self.kappa * np.log(omega)

        if self.Q_t != -self.Q:
            delta_pos = 1 / self.kappa - h[self.Q_t + self.Q - 1] + h[self.Q_t + self.Q]
        if self.Q_t != self.Q:
            delta_neg = 1 / self.kappa - h[self.Q_t + self.Q + 1] + h[self.Q_t + self.Q]

        if self.Q_t == -self.Q:
            d_ask = np.Inf
            d_bid = delta_neg
        elif self.Q_t == self.Q:
            d_ask = delta_pos
            d_bid = np.Inf
        else:
            d_ask = delta_pos
            d_bid = delta_neg

        action = np.array([d_bid, d_ask])

        return action


    def discrete_analytically_optimal(self):

        action = np.rint(self.calc_analytically_optimal() / self.dp) - self.min_dp

        return action


    def round_to_tick(self, p):

        return np.round(p / self.dp, decimals=0) * self.dp


    def transform_action(self, action):

        return (action + self.min_dp) * np.array([-1, 1]) * self.dp


    def step(self, action):

        self.t += self.dt

        self.update_price()


        if self.analytical:
            [self.mm_bid, self.mm_ask] = self.mid + np.array([-1, 1]) * action
        else:
            [self.mm_bid, self.mm_ask] = self.mid + self.transform_action(action)

        if self.debug:
            print("Starting volume for time step:", self.Q_t)
            print("The mid price is:", self.mid)
            print("The action is:", action)
            print("The choice is:", self.mm_bid, "|", self.mm_ask)


        if self.t <= self.T:

            n_MO_buy = np.random.poisson(self.lambda_neg)
            n_MO_sell = np.random.poisson(self.lambda_pos)

            # The probability that the orders get executed
            p_MO_buy = np.exp(-self.kappa * (self.mm_ask - self.mid))
            p_MO_sell = np.exp(-self.kappa * (self.mid - self.mm_bid))

            # Sample the number of orders executed
            n_exec_MO_buy = np.random.binomial(n_MO_buy, p_MO_buy)
            n_exec_MO_sell = np.random.binomial(n_MO_sell, p_MO_sell)


            if n_exec_MO_buy * n_exec_MO_sell > 0:
                self.X_t += (self.mm_ask - self.mm_bid) * np.min([n_exec_MO_buy, n_exec_MO_sell])


            n_MO_net = n_exec_MO_sell - n_exec_MO_buy


            if n_MO_net != 0:  
                if n_MO_net + self.Q_t > self.Q:  # long inventory limit breach
                    self.breach = self.breach_penalty_function(n_MO_net + self.Q_t - self.Q)
                    n_MO_net = self.Q - self.Q_t  # the maximum allowed net increase is given by Q - Q_t
                    n_exec_MO_sell -= (n_MO_net + self.Q_t - self.Q)
                elif n_MO_net + self.Q_t < - self.Q:  # short inventory limit breach
                    self.breach = self.breach_penalty_function(-self.Q - (n_MO_net + self.Q_t))
                    n_MO_net = - self.Q - self.Q_t  # the maximum allowed net decrease is given by -Q + Q_t
                    n_exec_MO_buy -= (- self.Q - (n_MO_net + self.Q_t))
                else:
                    self.breach = False



                # Step 3: add cash from net trading
                if n_MO_net > 0:
                    self.X_t -= self.mm_bid * n_MO_net
                elif n_MO_net < 0:
                    self.X_t -= self.mm_ask * n_MO_net

                self.Q_t += n_MO_net

            if self.debug:
                print("Arrvials:")
                print(n_MO_buy)
                print(n_MO_sell)

                print("\nProbabilities:")
                print(p_MO_buy)
                print(p_MO_sell)

                print("\nExecutions:")
                print(n_exec_MO_buy)
                print(n_exec_MO_sell)
                print("Net:", n_MO_net)
                print("Net:", n_exec_MO_sell - n_exec_MO_buy)

                print("\nX_t:", self.X_t)
                print("Q_t:", self.Q_t)

                print("_" * 20)

        # The time is up!
        if self.t == self.T:

            self.X_t += self.final_liquidation()

            self.Q_t = 0

        # ----- THE REWARD -----
        V_t_new = self.X_t + self.H_t()
        if self.t <= self.T:
            reward = self._get_reward(V_t_new)
        else:
            reward = 0


        self.V_t = V_t_new 

        if self.printing:
            print("The reward is:", reward)
            self.render()

        return self.state(), reward


    def _get_reward(self, V_t):


        if self.breaching_penalty:
            return self.reward_scale * (V_t - self.V_t) + self.inventory_penalty() - self.breach_penalty * self.breach
        else:
            return self.reward_scale * (V_t - self.V_t) + self.inventory_penalty()


    def H_t(self):

        return self.Q_t * self.mid


    def inventory_penalty(self):

        return - self.phi * (self.Q_t ** 2)


    def final_liquidation(self):

        return self.Q_t * (self.mid - self.alpha * self.Q_t)


    def reset(self):


        self.mid = self.mu
        self.Q_t = self.Q_0
        self.V_t = self.H_t()   # the value process involves no cash at the start
        self.t = 0
        self.X_t = 0            # the cash process




























    def render(self, mode='human', close=False):
        """
        Prints useful stats of the environment and the agent
        
        Parameters
        ----------
        mode : string
            ???
        close : bool
            ???
            
        Returns
        -------
        None
        """

        print(20 * '-')
        print(f'End of t = {self.t}')
        print(f'Current mid price: {np.round(self.mid, 2)}')
        print(f'Current held volume: {self.Q_t}')
        print(f'Current held value: {np.round(self.H_t(), 2)}')
        print(f'Current cash process: {np.round(self.X_t, 2)}')
        print(f'Current value process: {np.round(self.V_t, 2)}')
        print(20 * '-' + '\n')

"""# simple_model_mm_q_learning.py"""

from cProfile import label
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict
import random
import time as t
from datetime import timedelta
import os


def tabular_Q_learning(env, n=1e4, alpha_start=0.1, alpha_end=0.00005, alpha_cutoff=0.5,
                       epsilon_start=1, epsilon_end=0.05, epsilon_cutoff=0.25,
                       beta_start=1, beta_end=0.01, beta_cutoff=0.5,
                       gamma=1, exploring_starts=True):


    Q_tab = defaultdict(lambda: np.zeros((env.d, env.d)))

    state_action_count = defaultdict(lambda: np.zeros((env.d, env.d)))

    rewards_average = []  # to save the rewards

    Q_zero_average = []

    x_values = []

    reward_grouped = []
    Q_zero_grouped = []

    decreasing_factor = (alpha_end / alpha_start) ** (1 / n)

    alpha = alpha_start

    start_time = t.time()

    for episode in range(int(n)):
        # epsilon greedy policy
        epsilon = linear_decreasing(episode, n, epsilon_start, epsilon_end, epsilon_cutoff)

        # update learning rate
        # alpha = linear_decreasing(episode, n, alpha_start, alpha_end, alpha_cutoff)
        alpha = exponential_decreasing(alpha, factor=decreasing_factor)

        # update exploring starts
        beta = linear_decreasing(episode, n, beta_start, beta_end, beta_cutoff)

        # Exploring starts
        if exploring_starts is True:
            if random.random() < beta:
                env.Q_0 = env.observation_space.sample()[0]  # Sample Q_t from the observation space
            else:
                env.Q_0 = 0
        else:
            env.Q_0 = 0

        # Reset the environment
        env.reset()
        episode_reward = 0

        while env.t < env.T:  # As long as the episode isn't over
            state = env.state()

            explore = random.random() < epsilon

            if state not in list(Q_tab.keys()) or explore:  # Random if explore or unvisisted
                action = env.action_space.sample()
            else:  # Maximum otherwise
                action = np.array(np.unravel_index(Q_tab[state].argmax(), Q_tab[state].shape))

            new_state, action_reward = env.step(action)  # Get the new state and the reward

            action = tuple(action)  # To be able to use for indexing

            episode_reward += action_reward  # * (gamma ** env.t)  # Discounting with gamma

            state_action_count[state][action] += 1

            # Update the Q table
            Q_tab[state][action] = Q_tab[state][action] + alpha * (
                    action_reward + gamma * (np.max(Q_tab[new_state]) - Q_tab[state][action]))

        # Save the reward
        reward_grouped.append(episode_reward)

        # Save the best Q-value
        Q_zero_grouped.append(np.max(Q_tab[(0,0)]))

        # Printing every 20% of total episodes
        if (episode + 1) % (0.2 * n) == 0:
            percentage = "{:.0%}".format(episode / n)

            time_remaining = str(timedelta(seconds=round((t.time() - start_time) / (episode + 1) * (n - episode - 1), 2)))

            print("\tEpisode", episode + 1, "(" + percentage + "),", time_remaining, "remaining of this run")

        if episode % (max(n / 1e4, 5)) == 0:
            rewards_average.append(np.mean(reward_grouped))

            Q_zero_average.append(np.mean(Q_zero_grouped))

            x_values.append(episode)

            reward_grouped = []
            Q_zero_grouped = []


    return Q_tab, rewards_average, Q_zero_average, x_values


def exponential_decreasing(value, factor = 0.9999):

    return value * factor


def linear_decreasing(episode, n, start, end, cutoff):


    if episode < cutoff * n:
        value = start + (end - start) * episode / (cutoff * n)
    else:
        value = end

    return value


def save_Q(Q, args, n, rewards_average, Q_zero_average, x_values, suffix = "", folder_mode = False, folder_name = None):


    # Create the file
    if folder_mode:

        try:
            os.makedirs("results/simple_model/" + folder_name)
        except:
            print("THE FOLDER", folder_name, "ALREADY EXISTS")

        file_name = "results/simple_model/" + folder_name + "/" + fetch_table_name(args, n, suffix) + ".pkl"

    else:
        file_name = "results/simple_model/q_tables/" + fetch_table_name(args, n, suffix) + ".pkl"

    file = open(file_name, "wb")

    # Save the values in the file
    pickle.dump([dict(Q), args, n, rewards_average, Q_zero_average, x_values], file)
    file.close()

    return file_name


def load_Q(filename, default=True, folder_mode = False, folder_name = None):

    if folder_mode:
        results_folder = folder_name

    else:
        results_folder = "q_tables"
     
    # Load the file
    try:
        file = open("results/simple_model/" + results_folder + "/" + filename + ".pkl", "rb")
    except:
        file = open(filename, "rb")

    Q_raw, args, n, rewards_average, Q_zero_average, x_values = pickle.load(file)

    # If we don't want a defaultdict, just return a dict
    if not default:
        return Q_raw

    # Find d
    dim = Q_raw[(0, 0)].shape[0]

    # Transform to a default_dict
    Q_loaded = defaultdict(lambda: np.zeros((dim, dim)))
    Q_loaded.update(Q_raw)

    return Q_loaded, args, n, rewards_average, Q_zero_average, x_values


def fetch_table_name(args, n, suffix = ""):

    # Creates the table names based on the chosen parameters
    return "Q_" + "_".join(
        [list(args.keys())[i] + str(list(args.values())[i]) for i in range(len(args.values()))]) + "_n" + str(int(n)) + "_" + str(suffix)


def _plot_rewards_helper(x, smoothing=0.99):


    smooth_rewards = np.zeros(len(x))

    smooth_rewards[0] = x[0]

    for n in range(len(smooth_rewards) - 1):
        smooth_rewards[n + 1] = smoothing * smooth_rewards[n] + (1 - smoothing) * x[n + 1]

    return smooth_rewards


def plot_rewards(rewards, x_values):

    smoothing = 0.999

    average_rewards = _plot_rewards_helper(rewards, smoothing)

    plt.figure()
    plt.plot(x_values, average_rewards, label="reward")
    plt.ylabel("smoothed reward")
    plt.xlabel("episode")
    plt.title("Smoothed rewards during training with factor " + str(smoothing))
    plt.legend()
    plt.show()


def plot_Q_zero(Q_zero, x_values):

    plt.figure()
    plt.plot(x_values, Q_zero, label="Q[(0,0)]")
    plt.ylabel("reward of best action")
    plt.xlabel("episode")
    plt.title("Best Q-value from the start for every epsiode")
    plt.legend()
    plt.show()


def save_parameters(model_parameters, Q_learning_parameters, folder_name):

    file_path = "results/simple_model/" + folder_name + "/parameters.txt"

    with open(file_path, "w") as f:

        f.write("MODEL PARAMETERS\n")
        for key in list(model_parameters.keys()):
            f.write(str(key) + " : " + str(model_parameters[key]) + "\n")

        f.write("\nQ-LEARNING PARAMETERS\n")
        for key in list(Q_learning_parameters.keys()):
            f.write(str(key) + " : " + str(Q_learning_parameters[key]) + "\n")


def Q_learning_multiple(args, Q_learning_args, n = 1e5, n_runs = 5, folder_mode = True, folder_name = None):

    suffixes = np.arange(n_runs) + 1

    Q_learning_args["n"] = n

    file_names = []

    start_time = t.time()

    for suffix in suffixes:
        print("RUN", suffix, "IN PROGRESS...")
        start_time_sub = t.time()

        env = SimpleEnv(**args, printing=False, debug=False, breach_penalty_function = np.abs)

        Q_tab, rewards_average, Q_zero_average, x_values = tabular_Q_learning(env, **Q_learning_args)

        file_names.append(save_Q(Q_tab, args, n, rewards_average, Q_zero_average, x_values, suffix, folder_mode=folder_mode, folder_name=folder_name))

        # Calculate run time for the single run
        run_time = str(timedelta(seconds=round(t.time() - start_time_sub, 2)))
        print("...FINISHED IN", run_time)

        # Calculate the remaining time
        if suffix < len(suffixes):
            remaining_time = str(timedelta(seconds=round((len(suffixes)-suffix) * (t.time() - start_time_sub), 2)))
            print(remaining_time, "REMAINING OF THE TRAINING")

        print("="*40)

    # Print the total training time for all runs
    total_time = str(timedelta(seconds=round(t.time() - start_time, 2)))
    print("FULL TRAINING COMPLETED IN", total_time)

    if folder_mode:
        save_parameters(args, Q_learning_args, folder_name)

    return file_names

"""# simple_model_evaluation.py"""

import numpy as np
import matplotlib.pyplot as plt
from tabulate import tabulate
from collections import defaultdict


def evaluate_Q_matrix(matrix_path, n, folder_mode=False, folder_name=None, Q_tab = None, args = None):
    """
    returns the rewards for n runs based on a given Q-table
    Parameters
    ----------
    matrix_path : str
        the file path for the Q-table to be evaluated
    n : int
        how many episodes that will be simulated
    folder_mode : bool
        whether or not things should be loaded/saved to files
    folder_name : str
        where files are saved
    Q_tab : object
        a Q-table, if not given, one will be loaded
    args : dict
        a dictionary with arguments used for the environment
    Returns
    -------
    rewards : list
        a list of all the simulated rewards
    opt_action : tuple
        the best action at (0,0)
    Q_star : float
        the state-value at (0,0)
    """

    if Q_tab == None:
        Q_tab, args, _, _, _, _ = load_Q(matrix_path, folder_mode=folder_mode, folder_name=folder_name)

    env = SimpleEnv(**args, printing=False, debug=False, analytical=False)

    rewards = list()

    for _ in range(int(n)):

        env.reset()
        disc_reward = 0

        while env.t < env.T:
            state = env.state()
            action = np.array(np.unravel_index(Q_tab[state].argmax(), Q_tab[state].shape))

            _, action_reward = env.step(np.array(action))  # Get the new state and the reward

            disc_reward += action_reward  # * (gamma ** env.t)  # Discounting with gamma

        rewards.append(disc_reward)

    opt_action = np.unravel_index(Q_tab[(0,0)].argmax(), Q_tab[(0,0)].shape)
    Q_star = Q_tab[(0,0)][opt_action]

    return rewards, opt_action, Q_star


def evaluate_analytical_strategy(args_environment, args_generating, n=1000, discrete=True):
    """
    returns the rewards for n runs based on the analytically optimal strategy
    Parameters
    ----------
    args_environment : dict
        the parameters used for the environment
    args_generating : dict
        the parameters used for generating the strategies
    n : int
        how many episodes that will be simulated
    discrete : bool
        wheter or not the solution should be discretized
    Returns
    -------
    rewards : list
        a list of all the simulated rewards
    """

    env = SimpleEnv(**args_environment, printing=False, debug=False, analytical=True)

    data_bid = generate_optimal_depth(**args_generating, bid=True, discrete=discrete)
    data_ask = generate_optimal_depth(**args_generating, bid=False, discrete=discrete)

    rewards = list()

    Q = 3 # args_environment['Q']
    
    for _ in range(int(n)):

        env.reset()
        disc_reward = 0

        while env.t < env.T:
            state = env.state()
            action = data_bid[state[0] + Q, env.t], data_ask[state[0] + Q, env.t]

            _, action_reward = env.step(np.array(action))  # Get the new state and the reward

            disc_reward += action_reward  # * (gamma ** env.t)  # Discounting with gamma

        rewards.append(disc_reward)

    return rewards


def evaluate_constant_strategy(args_environment, n=1000, c=2):
    """
    returns the rewards for n runs based on the constant strategy - c ticks from the mid price
    Parameters
    ----------
    args_environment : dict
        the parameters used for the environment
    n : int
        how many episodes that will be simulated
    c : int
        the number of ticks away from the mid price
    Returns
    -------
    rewards : list
        a list of all the simulated rewards
    """

    env = SimpleEnv(**args_environment, printing=False, debug=False, analytical=False)

    rewards = list()

    for _ in range(int(n)):

        env.reset()
        disc_reward = 0

        while env.t < env.T:
            action = np.array([c, c]) - env.min_dp

            _, action_reward = env.step(np.array(action))  # Get the new state and the reward

            disc_reward += action_reward  # * (gamma ** env.t)  # Discounting with gamma

        rewards.append(disc_reward)

    return rewards


def evaluate_random_strategy(args_environment, n=1000):
    """
    returns the rewards for n runs based on a random strategy
    Parameters
    ----------
    args_environment : dict
        the parameters used for the environment
    n : int
        how many episodes that will be simulated
    Returns
    -------
    rewards : list
        a list of all the simulated rewards
    """

    env = SimpleEnv(**args_environment, printing=False, debug=False, analytical=False)

    rewards = list()

    for _ in range(int(n)):

        env.reset()
        disc_reward = 0

        while env.t < env.T:
            action = env.action_space.sample()

            _, action_reward = env.step(np.array(action))  # Get the new state and the reward

            disc_reward += action_reward  # * (gamma ** env.t)  # Discounting with gamma

        rewards.append(disc_reward)

    return rewards


def evaluate_strategies(path, n=1000, T=5, Q=3, dp=0.01, alpha=1e-4, phi=1e-5, min_dp=0, d=4, c=2):
    """
    returns the rewards for n runs based on a random strategy
    Parameters
    ----------
    path : str
        where the Q-table is saved
    n : int
        how many episodes that will be simulated
    T : int
        the length of the episode
    Q : int
        the maximum allowed inventory - (-Q,Q)
    dp : float
        the tick size
    alpha : float
        penalty term at liquidation
    phi : float
        the running inventory penalty
    min_dp : int
        the minimum quote depth
    d : int
        the number of depths the MM can quote at
    c : int
        the depth of the constant strategy
    Returns
    -------
    None
    """

    args_environment = {"T": T, "Q": Q, "dp": dp, "alpha": alpha, "phi": phi, "use_all_times": True, "min_dp": min_dp,
                        "d": d, "breaching_penalty": False}
    args_generating = {"T": T, "Q": Q, "dp": dp, "phi": phi}

    # Get the analytical solutions
    rewards_analytical_discrete = evaluate_analytical_strategy(args_environment, args_generating, n=n, discrete=True)
    rewards_analytical_continuous = evaluate_analytical_strategy(args_environment, args_generating, n=n, discrete=False)

    # Get the constant rewards
    rewards_constant = evaluate_constant_strategy(args_environment, n=n, c=c)

    # Get the random rewards
    rewards_random = evaluate_random_strategy(args_environment, n=n)

    # Get the Q-learning rewards
    rewards_Q_learning, _, _ = evaluate_Q_matrix(path, n=n)

    data = [rewards_analytical_discrete,
            rewards_analytical_continuous,
            rewards_constant,
            rewards_random,
            rewards_Q_learning]

    labels = ["analytical_discrete",
              "analytical_continuous",
              "constant (d=" + str(c) + ")",
              "random",
              "Q_learning"]

    headers = ['strategy', 'mean reward', 'std reward']
    rows = []
    for i, label in enumerate(labels):
        rows.append([label, np.mean(data[i]), np.std(data[i])])

    print("Results:\n")
    print(tabulate(rows, headers=headers))

    plt.figure(figsize=(12,5))
    plt.boxplot(data, labels=labels)
    plt.ylabel("reward")
    plt.show()


def evaluate_strategies_multiple_Q(file_names, args, mean_rewards, Q_mean, n_test = 1e2, c = 2, 
                                    folder_mode=False, folder_name=None, save_mode = False):
    """
    compares different strategies with boxplots and a table with mean and rewards
    Parameters
    ----------
    file_names : list
        a list with the paths for the Q-tables
    args : dict
        the parameters used for the environment
    mean_rewards : list
        the mean rewards of the Q-tables tested
    Q_mean : dict
        the mean Q-table
    n_test : int
        the number of episodes the strategies are evaluated for
    c : int
        the number of ticks the constant strategies uses
    folder_mode : bool
        whether or not things should be loaded/saved to files
    folder_name : str
        where files are saved
    save_mode : bool
        whether or not figures and tables should be saved
    Returns
    -------
    None
    """

    args_environment = args
    args_generating = {"T": args["T"], "Q": 3, "dp": args["dp"], "phi": args["phi"]}

    # Get the analytical solutions
    rewards_analytical_discrete = evaluate_analytical_strategy(args_environment, args_generating, n=n_test, discrete=True)
    rewards_analytical_continuous = evaluate_analytical_strategy(args_environment, args_generating, n=n_test, discrete=False)

    # Get the constant rewards
    rewards_constant = evaluate_constant_strategy(args_environment, n = n_test, c=c)

    # Get the random rewards
    rewards_random = evaluate_random_strategy(args_environment, n=n_test)

    # Get the best Q-learning rewards
    best_idx = np.argmax(mean_rewards)
    rewards_Q_learning_best, _, _ = evaluate_Q_matrix(file_names[best_idx], n=n_test, folder_mode=folder_mode, folder_name=folder_name)

    # Get the average Q-learning rewards
    rewards_Q_learning_average, _, _ = evaluate_Q_matrix(None, n=n_test, folder_mode=folder_mode, folder_name=folder_name, Q_tab=Q_mean, args=args)

    data = [rewards_analytical_discrete,
            rewards_analytical_continuous,
            rewards_constant,
            rewards_random,
            rewards_Q_learning_best,
            rewards_Q_learning_average]

    labels = ["analytical_discrete",
              "analytical_continuous",
              "constant (d=" + str(c) + ")",
              "random",
              "Q_learning (best run)",
              "Q_learning (average)"]

    headers = ['strategy', 'mean reward', 'std reward']
    rows = []
    for i, label in enumerate(labels):
        rows.append([label, np.mean(data[i]), np.std(data[i])])

    if save_mode:
        with open("results/simple_model/" + folder_name + "/" "table_benchmarking", "w") as f:
            f.write(tabulate(rows, headers=headers))
    else:
        print("Results:\n")
        print(tabulate(rows, headers=headers))

    plt.figure(figsize=(12,5))
    plt.boxplot(data, labels=labels)
    plt.title("Comparison of different strategies")
    plt.ylabel("reward")

    if save_mode:
        plt.savefig("results/simple_model/" + folder_name + "/" "box_plot_benchmarking")
        plt.close()
    else:
        plt.show()


def compare_Q_learning_runs(file_names, n_test = 1e2, folder_mode=False, folder_name=None, save_mode = False):
    """
    compares different Q-learning runs with boxplots and a table with mean and rewards
    Parameters
    ----------
    file_names : list
        a list with the paths for the Q-tables
    n_test : int
        the number of episodes the strategies are evaluated for
    folder_mode : bool
        whether or not things should be loaded/saved to files
    folder_name : str
        where files are saved
    save_mode : bool
        whether or not figures and tables should be saved
    Returns
    -------
    mean_reward : np.array
        return the mean reward of each run
    """

    data = []
    actions = []
    q_values = []

    for file_name in file_names:
        reward, action, q_value = evaluate_Q_matrix(file_name, n=n_test, folder_mode=folder_mode, folder_name=folder_name)
        data.append(reward)
        actions.append(action)
        q_values.append(q_value)

        labels = ["run " + str(i + 1) for i in range(len(file_names))]
    
    headers = ['run', 'mean reward', 'std reward', 'opt action', 'q-value']
    rows = []

    for i, label in enumerate(labels):
        rows.append([label, np.mean(data[i]), np.std(data[i]), actions[i], q_values[i]])

    if save_mode:
        with open("results/simple_model/" + folder_name + "/" "table_different_runs", "w") as f:
            f.write(tabulate(rows, headers=headers))
    else:
        print("Results:\n")
        print(tabulate(rows, headers=headers))

    plt.figure(figsize=(12,5))
    plt.boxplot(data, labels=labels)
    plt.title("Comparison of different Q-learning runs")
    plt.ylabel("reward")

    if save_mode:
        plt.savefig("results/simple_model/" + folder_name + "/" "box_plot_different_runs")
        plt.close()
    else:
        plt.show()

    return np.mean(data, axis=1)


def plot_rewards_multiple(file_names, folder_mode=False, folder_name=None, save_mode = False):
    """
    plots average rewards and Q-values with CI for several runs
    Parameters
    ----------
    file_names : list
        a list with the paths for the rewards and Q-values
    folder_mode : bool
        whether or not things should be loaded/saved to files
    folder_name : str
        where files are saved
    save_mode : bool
        whether or not figures and tables should be saved
    Returns
    -------
    None
    """
    reward_matrix = []
    Q_zero_matrix = []
    
    # Open and save the values
    for file_name in file_names:
        _, _, _, rewards_average, Q_zero_average, x_values = load_Q(file_name, folder_mode=folder_mode, folder_name=folder_name)

        reward_matrix.append(rewards_average)
        Q_zero_matrix.append(Q_zero_average)

    reward_matrix = np.array(reward_matrix)
    Q_zero_matrix = np.array(Q_zero_matrix)

    # Calculate mean, std and area
    reward_mean = np.mean(reward_matrix, axis=0)
    reward_std = np.std(reward_matrix, axis=0)

    Q_zero_mean = np.mean(Q_zero_matrix, axis=0)
    Q_zero_std = np.std(Q_zero_matrix, axis=0)

    reward_area = np.array([reward_std, -reward_std]) + reward_mean
    Q_zero_area = np.array([Q_zero_std, -Q_zero_std]) + Q_zero_mean

    # Plotting
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))

    # Plot the rewards
    ax1.fill_between(x_values, reward_area[0, :], reward_area[1, :], alpha = 0.3, color="purple", label="±$\sigma$")
    ax1.plot(x_values, reward_mean, linewidth=0.2, color="purple", label="mean reward")
    ax1.set_xlabel('episode')
    ax1.set_xticks(np.linspace(0, x_values[-1], 6))
    ax1.set_ylabel('reward')
    ax1.set_title('average reward during training')

    # Plot the Q-values
    ax2.fill_between(x_values, Q_zero_area[0, :], Q_zero_area[1, :], alpha = 0.3, color="purple", label="±$\sigma$")
    ax2.plot(x_values, Q_zero_mean, linewidth=0.2, color="purple", label="mean Q[(0,0)]-value")
    ax2.set_xlabel('episode')
    ax2.set_xticks(np.linspace(0, x_values[-1], 6))
    ax2.set_ylabel('Q[(0,0)]')
    ax2.set_title('average Q[(0,0)] during training')

    ax1.legend()
    ax2.legend()

    if save_mode:
        plt.savefig("results/simple_model/" + folder_name + "/" "results_graph")
        plt.close()
    else:
        plt.show()


def calculate_mean_Q(file_names, folder_mode=False, folder_name=None):
    """
    calculate an average Q-table based on several
    Parameters
    ----------
    file_names : list
        a list with the paths for the Q-tables
    folder_mode : bool
        whether or not things should be loaded/saved to files
    folder_name : str
        where files are saved
    Returns
    -------
    Q_mean : defaultdict
        a defaultdict with states as keys and average Q-values as values
    Q_tables : list
        a list with defaultdicts with states as keys and Q-values as values
    """

    # Fetch the model parameters
    _, args, _, _, _, _ = load_Q(file_names[0], folder_mode=folder_mode, folder_name=folder_name)

    Q_mean = defaultdict(lambda: np.zeros((args["d"], args["d"])))

    Q_tables = []

    # Load the files
    for file_name in file_names:
        Q_tables.append(load_Q(file_name, folder_mode=folder_mode, folder_name=folder_name)[0])

    # Calculate the mean
    for state in list(Q_tables[0].keys()):
        Q_mean[state] = np.mean([Q_tables[i][state] for i in range(len(Q_tables))], axis = 0)

    return Q_mean, Q_tables


def calculate_std_Q(Q_mean, Q_tables):
    """
    calculate the std of the mean optimal action of several runs
    Parameters
    ----------
    Q_mean : defaultdict
        a defaultdict with states as keys and average Q-values as values
    Q_tables : list
        a list with defaultdicts with states as keys and Q-values as values
    Returns
    -------
    Q_std : defaultdict
        a defaultdict with states as keys and std of Q-values as values
    """

    Q_std = Q_mean

    for state in list(Q_mean.keys()):
        # Find the optimal action based on mean
        optimal_action = np.array(np.unravel_index(Q_mean[state].argmax(), Q_mean[state].shape))

        # Calculate the standard deviation of the q-value of that action
        Q_std[state] = np.std([Q_tables[i][state][tuple(optimal_action)] for i in range(len(Q_tables))])
    
    return Q_std


def args_to_file_names(args, n_runs, n):
    """
    returns a list of file_names based on model parameters
    Parameters
    ----------
    args : dict
        a dict with model parameters
    n_runs : int
        the number of different runs performed
    n : int
        the number of episodes each runs is trained for
    Returns
    -------
    file_names : list
        a list with strings of file_names
    """

    suffixes = np.arange(n_runs) + 1

    file_names = []

    for suffix in suffixes:
        file_names.append(fetch_table_name(args, n, suffix))

    return file_names


def Q_learning_comparison(n_train = 1e2, n_test = 1e1, n_runs = 2, file_names = None, args = None, 
                            Q_learning_args = None, folder_mode = False, folder_name = None, save_mode = False):
    """
    runs tabular Q-learning several times and compares them against eachother and other strategies
    also does some plotting
    Parameters
    ----------
    n_train : int
        the number of episodes the Q-learning is run for
    n_test : int
        the number of episodes the strategies are evaluated for
    n_runs : int
        how many times the Q-learning is performed
    file_names : list
        a list with strings of the save locations of Q-tables
    args : dict
        the parameters used for the environment
    Q_learning_args : dict
        the parameters used for the Q-learning
    folder_mode : bool
        whether or not things should be loaded/saved to files
    folder_name : str
        where files are saved
    save_mode : bool
        whether or not figures and tables should be saved
    Returns
    -------
    None
    """
    
    if file_names == None:
        file_names = Q_learning_multiple(args, Q_learning_args, n_train, n_runs, folder_mode=folder_mode, folder_name=folder_name)

    # Using mean Q instead
    Q_mean, Q_tables = calculate_mean_Q(file_names, folder_mode=folder_mode, folder_name=folder_name)

    plot_rewards_multiple(file_names, folder_mode=folder_mode, folder_name=folder_name, save_mode=save_mode)

    mean_rewards = compare_Q_learning_runs(file_names, n_test, folder_mode=folder_mode, folder_name=folder_name, save_mode=save_mode)

    evaluate_strategies_multiple_Q(file_names, args, mean_rewards, Q_mean, n_test, folder_mode=folder_mode, folder_name=folder_name, save_mode=save_mode)

    env = SimpleEnv(**args, printing=False, debug=False, analytical=False)

    file_path = "results/simple_model/" + folder_name + "/" if save_mode else None

    Q_mean = remove_last_t(Q_mean, env.T)

    show_Q(Q_mean, env, file_path = file_path)
    heatmap_Q(Q_mean, file_path = file_path)

    heatmap_Q_n_errors(Q_mean.copy(), Q_tables.copy(), n_unique = True, file_path = file_path)
    heatmap_Q_n_errors(Q_mean.copy(), Q_tables.copy(), n_unique = False, file_path = file_path)

    Q_std = calculate_std_Q(Q_mean, Q_tables)
    heatmap_Q_std(Q_std, file_path = file_path)


def get_args_from_txt(folder_name):
    """
    fetches arguments from a parameters.txt file
    Parameters
    ----------
    folder_name : str
        where the txt file is saved
    Returns
    -------
    args : dict
        the parameters used for the environment
    """

    f = open("results/simple_model/" + folder_name + "/parameters.txt")
    lines = f.readlines()

    args = {}

    for line in lines:
        if line == "\n":
            return args

        if line != "MODEL PARAMETERS\n":
            key, value = line.split(":")
            key = key.strip()
            value = value.strip()

            if value in ["True", "False"]:
                value = bool(value)
            else:
                value = float(value)
                if value == int(value):
                    value = int(value)

            args[key] = value

"""# Q-learning_in_the_simple_probabilistic_model.ipynb"""

model_params = {
                "d": 4,         # the number of different quotation depths that can be chosen from
                "T": 5,        # the length of the episode
                "dp": 0.01,     # the tick size
                "min_dp": 0,    # the minimum number of ticks from the mid price that is allowed to put prices at
                "phi": 1e-4     # the running inventory penalty
}

Q_learning_params = {
        # epsilon-greedy values (linear decay)
        "epsilon_start": 1,
        "epsilon_end": 0.05,
        "epsilon_cutoff": 0.5,

        # learning-rate values (exponential decay)
        "alpha_start": 0.5,
        "alpha_end": 0.001,
        "alpha_cutoff": None,

        # exploring starts values (linear decay)
        "beta_start": 1,
        "beta_end": 0.05,
        "beta_cutoff": 0.5,
        "exploring_starts": True
}

hyperparams = {
        "n_train" : 1e5,
        "n_test" : 1e4,
        "n_runs" : 10
}

# naming the folder where the results will be saved
folder_mode = True
folder_name = "spm_example"
save_mode = True

Q_learning_comparison(
    **hyperparams,
    args                = model_params,
    Q_learning_args     = Q_learning_params,
    folder_mode         = folder_mode,
    folder_name         = folder_name,
    save_mode           = save_mode
)
